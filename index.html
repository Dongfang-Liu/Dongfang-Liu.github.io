<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js">
<!--<![endif]-->

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Dongfang Liu's website</title>
    <meta name="description" content="UTSA">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="css/normalize.css">
    <link rel="stylesheet" href="css/font-awesome.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/templatemo-style.css">
    <script src="js/vendor/modernizr-2.6.2.min.js"></script>
</head>

<body>
    <!--[if lt IE 7]>
            <p class="browsehappy">You are using an <strong>outdated</strong> browser. Please upgrade your browser to improve your experience.</p>
        <![endif]-->

    <div class="responsive-header visible-xs visible-sm">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <div class="top-section">
                        <div class="profile-image">
                            <img src="img/dongfang.jpg" backgroundSize="contain" alt="Dongfang Liu">
                        </div>
                        <div class="profile-content">
                            <h3 class="profile-title">Dongfang Liu</h3>
                            <p class="profile-description">Rochester Institute of Technology</p>
                            <p class="profile-description">dongfang.liu AT rit.edu</p>
                        </div>
                    </div>
                </div>
            </div>
            <a href="#" class="toggle-menu"><i class="fa fa-bars"></i></a>
            <div class="main-navigation responsive-menu">
                <ul class="navigation">
                    <li><a href="#top"><i class="fa fa-home"></i>Home</a></li>
                    <li><a href="#about"><i class="fa fa-user"></i>About Me</a></li>
                    <li><a href="#research"><i class="fa fa-newspaper-o"></i>Research</a></li>
                    <li><a href="#publications"><i class="fa fa-book"></i>Publications</a></li>
                </ul>
            </div>
        </div>
    </div>

    <!-- SIDEBAR -->
    <div class="sidebar-menu hidden-xs hidden-sm">
        <div class="top-section">
            <div class="profile-image">
                <img src="img/dongfang.jpg" alt="Dongfang Liu">
            </div>
            <h3 class="profile-title">Dr. Dongfang Liu</h3>
            <p class="profile-description">Assistant professor @ RIT</p>
            <p class="profile-description">Ph.D from Purdue University (2021)</p>
            <p class="profile-description">dongfang.liu AT rit.edu</p>
        </div> <!-- top-section -->
        <div class="main-navigation">
            <ul class="navigation">
                <li><a href="#top"><i class="fa fa-home"></i>Home</a></li>
                <li><a href="#about"><i class="fa fa-user"></i>About Me</a></li>
                <li><a href="#interests"><i class="fa fa-newspaper-o"></i>Research Interests</a></li>
                <li><a href="#projects"><i class="fa fa-file-o"></i>Research Projects</a></li>
                <li><a href="#services"><i class="fa fa-cogs"></i>Services</a></li>
                <li><a href="#publications"><i class="fa fa-book"></i>Publications</a></li>
            </ul>
        </div> <!-- .main-navigation -->
        <!--<div class="social-icons">-->
        <!--<ul>-->
        <!--<li><a href="#"><i class="fa fa-facebook"></i></a></li>-->
        <!--<li><a href="#"><i class="fa fa-linkedin"></i></a></li>-->
        <!--<li><a href="https://scholar.google.com/citations?user=_Voig00AAAAJ&hl=en" target="_blank"><i class="fa fa-google"></i></a></li>-->
        <!--<li><a href="#"><i class="fa fa-rss"></i></a></li>-->
        <!--</ul>-->
        <!--</div> &lt;!&ndash; .social-icons &ndash;&gt;-->
    </div> <!-- .sidebar-menu -->

    <div class="banner-bg" id="top">
        <!--<div class="banner-overlay"></div>-->
        <!-- <div class="welcome-text">
                <h2>Boiler up</h2>
            </div> -->
    </div>

    <!-- MAIN CONTENT -->
    <div class="main-content">
        <div class="fluid-container">

            <div class="content-wrapper">

                <!-- ABOUT -->
                <div class="page-section" id="about">
                    <div class="row">
                        <div class="col-md-12">
                            <h4 class="widget-title">About Me</h4>
                            <p> I am an Assistant Professor in the Department of Computer Engineering and an affiliated faculty member with Computer Science department.
                                My research interests primarily revolve around general-purpose AI. Over the past few years, 
                                I have had the pleasure of working alongside a cohort of exceptionally bright and talented students who have accompanied
                                me on this academic journey. Sometimes, they listen to me. I have a charming feline companion üêà that graces my life, Mr. Tiger, who brings a lot of joy to the family. 
                                <hr>
                        </div>
                    </div> <!-- #about -->
                </div>

                <!-- Research -->
                <div class="page-section" id="interests">
                    <h4 class="widget-title">Research Interests</h4>
                    <div class="row">
                        <div class="col-md-12">
                            <div class="project-item">
                                <!--<b>Record and Replay</b>-->
                                <!--<p>
                        Reproducing errors of multithreaded programs is very challenging due to many intrinsic and external non-deterministic factors. Existing RnR systems achieve significant progress in terms of performance overhead, but none target the in-situ setting, in which replay occurs within the same process as the recording process. Also, most cannot achieve identical replay, which may prevent the reproduction of some errors.
                        </p>-->
                                <p>
                                  My research endeavors have been dedicated to the development of <b>general intelligence for X</b> tailored to 
                                    address interdisciplinary challenges (e.g., autonomous driving, cancer diagnosis, and sustainable energy). Through 
                                    these multidisciplinary investigations, I strive to contribute to the advancement of knowledge and the resolution 
                                    of pressing issues across diverse disciplines (e.g., transportation, health care, and inertial confinement fusion). These endeavors have garnered support 
                                    from National Science Foundation (NSF) and indurstial partners. By harnessing the power of AI, I aim to unravel complexities, 
                                    devise innovative strategies, and ultimately make substantial contributions to the betterment of society. I am
                                    continuously looking for <b>
                                        <font color="#ff4500"> doctoral students</font>
                                    </b> to work with my research projects. If you want to develop awesome AI tools to address
                                    real-world problems, I think we need to talk üòÑ.
                                </p>
                            </div>
                        </div>
                    </div>
                </div> <!-- .research-holder -->
                <hr>


                <!-- Research Projects-->
                <div class="page-section" id="projects">
                    <h4 class="widget-title">
                        Research Projects
                        <p id="readMoreProjects"><i class="down"></i></p>
                    </h4>
                    <div class="project-container">
                        <div class="project-item top">
                            <!-- <div class="project-image"><img src="img/visual rec.png" /></div> -->
                            <div class="tab-wrap">
                                <div class="tab clearfix">
                                    <div class="tab-item cur">
                                        <a href="javascript:;"></a>
                                    </div>
                                    <div class="tab-item">
                                        <a href="javascript:;"></a>
                                    </div>
                                    <div class="tab-item">
                                        <a href="javascript:;"></a>
                                    </div>
                                </div>
                                <div class="page">
                                    <div class="page-item active">
                                        <img src="img/transflow.png" />
                                    </div>
                                    <div class="page-item">
                                        <img src="img/visual rec.png" />
                                    </div>
                                    <div class="page-item">
                                        <img src="img/Selfadversial.png" />
                                    </div>
                                </div>
                            </div>
                            <div class="project-info">
                                <b><a href="./projects/project1.html"><b>Project 1</b>: Vision-Anchored Automation of
                                    Bird-Sized UAVs in Unknown Cluttered Indoor Environments</a></b></br>
                                <i>The research project aims to develop full automation for bird-sized UAVs using only an RGB-D camera, and seeks to answer fundamental questions for UAV automation, including constructing visual perception and developing visual navigation.</br>
                            </div>
                        </div>
                        <img class="nsf" src="./img/nsf.png">
                    </div>
                    <section class="paper-more" style="display: none;">
                        <div class="project-container">
                            <div class="project-item">
                                <!-- <div class="project-image"><img src="img/LearningtoGenerateQuestion.png" /></div>
                                <div class="project-info">
                                    <b><a href="./projects/project2.html"><b>Project 2</b>:  Vision-Anchored Automation of
                                        Bird-Sized UAVs in Unknown Cluttered Indoor Environments</a></b>
                                </div> -->
                                <!-- <div class="tab-wrap">
                                    <div class="tab clearfix">
                                        <div class="tab-item cur">
                                            <a href="javascript:;"></a>
                                        </div>
                                        <div class="tab-item">
                                            <a href="javascript:;"></a>
                                        </div>
                                        <div class="tab-item">
                                            <a href="javascript:;"></a>
                                        </div>
                                    </div>
                                    <div class="page">
                                        <div class="page-item active">
                                            <img src="img/transflow.png" />
                                        </div>
                                        <div class="page-item">
                                            <img src="img/visual rec.png" />
                                        </div>
                                        <div class="page-item">
                                            <img src="img/Selfadversial.png" />
                                        </div>
                                    </div>
                                </div>
                                <div class="project-info">
                                    <b><a href="./projects/project2.html"><b>Project 2</b>: Vision-Anchored Automation of
                                        Bird-Sized UAVs in Unknown Cluttered Indoor Environments</a></b></br>
                                    <i>The research project aims to develop full automation for bird-sized UAVs using only an RGB-D camera, and seeks to answer fundamental questions for UAV automation, including constructing visual perception and developing visual navigation.</br>
                                </div>
                            </div>
                            <img class="nsf" src="./img/nsf.png"> -->
                        </div>
                    </section>
                </div>
                <hr>

                <!-- Service -->
                <div class="page-section" id="services">
                    <h4 class="widget-title">Services</h4>
                    <div class="row">
                        <div class="col-md-12">
                            <div class="project-item">
                                <ul>
                                     <li><b>Area Chair</b>: The Conference on Computer Vision and Pattern Recognition (CVPR), 2023-
                                    <li><b>Senior PC</b>: The International Joint Conference on Artificial
                                        Intelligence (IJCAI), 2023-
                                    <li><b>Senior PC</b>: The Association for the Advancement of Artificial
                                        Intelligence (AAAI), 2023-
                                    <li><b>Associate editor</b>: IEEE Transactions on Circuits and Systems for Video
                                        Technology (TCSVT), 2023-</li>
                                    <li><b>Associate editor</b>: Neurocomputing, 2024-</li>
                                    <li><b>Associate editor</b>: Multimedia Tools and Applications (MTAP), 2023-</li>
                                    <li><b>Associate editor</b>: ACM Journal on Autonomous Transportation (JATS), 2023-</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                <hr>

                <!-- publications-->
                <div class="page-section" id="publications">
                    <h4 class="widget-title">Publications<p id="readMore"><i class="down"></i></p>
                    </h4>
                    <div class="paper-year">2025</div>
                    <div class="paper-item top">
                    <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>MEPT: Mixture of Expert Prompt Tuning as a Manifold Mapper</b></br>
                            <p><i>Runjia Zeng, Guangyan Sun, Qifan Wang, Tong Geng, Sohail Dianat, Xiaotian Han, Raghuveer Rao, Xueling Zhang, Cheng Han, Lifu Huang, <b>Dongfang Liu*</b></p>
                            <b>EMNLP 2025 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                    <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics</b></br>
                            <p><i>Taowen Wang, Cheng Han, James Chenhao Liang, Wenhao Yang, <b>Dongfang Liu*</b>, Luna Xinyu Zhang, Qifan Wang, Jiebo Luo, Ruixiang Tang</p>
                            <b>ICCV 2025 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                    <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Visual Agents as Fast and Slow Thinkers</b></br>
                            <p><i>Guangyan Sun, Mingyu Jin, Zhenting Wang, Cheng-Long Wang, Siqi Ma, Qifan Wang, Tong Geng, Ying Nian Wu, Yongfeng Zhang, <b>Dongfang Liu*</b></p>
                            <b>ICLR 2025 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                    <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Re-Imagining Multimodal Instruction Tuning: A Representation View</b></br>
                            <p><i>Yiyang Liu, James Chenhao Liang, Ruixiang Tang, Yugyung Lee, MAJID RABBANI, Sohail Dianat, Raghuveer Rao, Lifu Huang, <b>Dongfang Liu</b>, Qifan Wang, Cheng Han</p>
                            <b>ICLR 2025 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                    <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>DS-LLM: Leveraging Dynamical Systems to Enhance Both Training and Inference of Large Language Models</b></br>
                            <p><i>Ruibing Song, Chuan Liu, Chunshu Wu, Ang Li, <b>Dongfang Liu</b>, Ying Nian Wu, Tong Geng</p>
                            <b>ICLR 2025 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                    <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Diff-PIC: Revolutionizing Particle-In-Cell Nuclear Fusion Simulation with Diffusion Models</b></br>
                            <p><i>Chuan Liu, Chunshu Wu, shihui cao, Mingkai Chen, James Chenhao Liang, Ang Li, Michael Huang, Chuang Ren, Ying Nian Wu, <b>Dongfang Liu</b>, Tong Geng</p>
                            <b>ICLR 2025 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-year">2024</div>
                    <div class="paper-item top">
                    <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Visual Fourier Prompt Tuning</b></br>
                            <p><i>Runjia Zeng, Cheng Han, Qifan Wang, Chunshu Wu, Tong Geng, Lifu Huang, Ying Nian Wu, <b>Dongfang Liu*</b></p>
                            <b>NeurIPS 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                    <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Optical Flow as Spatial-Temporal Attention Learners</b></br>
                            <p><i>Yawen Lu, Cheng Han, Qifan Wang, Heng Fan, Zhaodan Kong, <b>Dongfang Liu*</b>, Yingjie Chen</p>
                            <b>TPAMI  2024</b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                    <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Diffusion-Inspired Truncated Sampler for Text-Video Retrieval</b></br>
                            <p><i>Jiamian Wang, Pichao Wang, <b>Dongfang Liu</b>, Qiang Guan, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao. </p>
                            <b>NeurIPS  2024</b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                    <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>MPT: Multimodal Prompt Tuning for Zero-shot Instruction Learning</b></br>
                            <p><i>Taowen Wang, Yiyang Liu, James Chenhao Liang, junhan zhao, Yiming Cui, Yuning Mao, Shaoliang Nie, Jiahao Liu, Fuli Feng, Zenglin Xu, Cheng Han, Lifu Huang, Qifan Wang, <b>Dongfang Liu*</b></p>
                            <b>EMNLP 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>EAVE: Efficient Product Attribute Value Extraction via Lightweight Sparse-layer Interaction</b></br>
                            <p><i>Li Yang, Qifan Wang, Jianfeng Chi, Jiahao Liu, Jingang Wang, Fuli Feng, Zenglin Xu, Yi Fang, Lifu Huang, <b>Dongfang Liu*</b></p>
                            <b>EMNLP 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Towards Automatic Oracle Prediction for AR testing: Assessing Virtual Object Placement Quality under Real-world Scenes</b></br>
                            <p><i>Xiaoyi Yang, Yuxing Wang, Tahmid Rafi, <b>Dongfang Liu</b>, Xiaoyin Wang, Xueling Zhang</p>
                            <b>ISSTA 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>UAV First-Person Viewers Are Radiance Field Learners</b></br>
                            <p><i>Liqi Yan, Qifan Wang, Junhan Zhao, Qiang Guan, Zheng Tang, Jianhui Zhang, <b>Dongfang Liu*</b></p>
                            <b>ECCV 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>AMD: Automatic Multi-step Distillation of Large-scale Vision Models</b></br>
                            <p><i>Cheng Han, Qifan Wang,  Sohail Dianat, Majid Rabbani, Raghuveer Rao, Yi Fang, Qiang Guan, Lifu Huang, <b>Dongfang Liu*</b></p>
                            <b>ECCV 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning</b></br>
                            <p><i>Shaohua Dong, Yunhe Feng, Qing Yang, Yan Huang, <b>Dongfang Liu</b>, Heng Fan</p>
                            <b>IROS 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Self-supervised Adversarial Training of  Monocular Depth Estimation against Physical-World Attacks</b></br>
                            <p><i>Zhiyuan Cheng, Cheng Han, James Liang, Qifan Wang, Xiangyu Zhang, <b>Dongfang Liu*</b></p>
                            <b>TPAMI 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Prototypical Transformer As Unified Motion Learners</b></br>
                            <p><i>Cheng Han, Yawen Lu, Guohao Sun, James Chenhao Liang, Zhiwen Cao, Qifan Wang, Qiang Guan, Sohail Dianat, Raghuveer Rao, Tong Geng, Zhiqiang Tao, <b>Dongfang Liu*</b></p>
                            <b>ICML 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks</b></br>
                            <p><i>Zhiyuan Cheng, Zhaoyi Liu, Tengda Guo, Shiwei Feng, <b>Dongfang Liu</b>, Mingjie Tang, Xiangyu Zhang</p>
                            <b>ICML 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>SmartFuse: Reconfigurable Smart Switches to Accelerate Fused Collectives in HPC Applications</b></br>
                            <p><i>Pouya Haghi, Cheng Tan, Anqi Guo, Chunshu Wu, <b>Dongfang Liu</b>, Ang Li, Anthony Skjellum, Tong Geng, Martin Herbordt</p>
                            <b>ICS 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval</b></br>
                            <p><i>Jiamian Wang, Guohao Sun, Pichao Wang, <b>Dongfang Liu</b>, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao</p>
                            <b>CVPR 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>ProMotion: Prototypes As Motion Learners</b></br>
                            <p><i>Yawen Lu, <b>Dongfang Liu*</b>, Qifan Wang, Cheng Han, Yiming Cui, Zhiwen Cao, Xueling Zhang, Yingjie Victor Chen, Heng Fan</p>
                            <b>CVPR 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Image Translation as Diffusion Visual Programmers</b></br>
                            <p><i>Cheng Han, James Chenhao Liang, Qifan Wang, MAJID RABBANI, Sohail Dianat, Raghuveer Rao, YingNian Wu, <b>Dongfang Liu*</b></p>
                            <b>ICLR 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Facing the Elephant in the Room: Visual Prompt Tuning or Full finetuning?</b></br>
                            <p><i>Cheng Han, Qifan Wang, Yiming Cui, Wenguan Wang, Lifu Huang, Siyuan Qi, <b>Dongfang Liu*</b></p>
                            <b>ICLR 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Fusion is Not Enough: Single Modal Attack on Fusion Models for 3D Object Detection</b></br>
                            <p><i>Zhiyuan Cheng, Hongjun Choi, Shiwei Feng, James Chenhao Liang, Guanhong Tao, <b>Dongfang Liu</b>, Michael Zuzak, Xiangyu Zhang</p>
                            <b>ICLR 2024 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-year">2023</div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>Reformulating Graph Kernels for Self-supervised Space-Time Correspondence Learning</b></br>
                            <p><i>Zheyun Qin, Xiankai Lu, <b>Dongfang Liu</b>, Xiushan Nie, Yilong Yin, Jianbing Shen, Alexander C. Loui.</p>
                            <b>TIP 2023 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models</b></br>
                            <p><i>Qifan Wang, Yuning Mao, Jingang Wang, Hanchao Yu, Shaoliang Nie, Sinong Wang, Fuli Feng, Lifu Huang, Xiaojun Quan, Zenglin Xu, 
                                    <b>Dongfang Liu*</b>.</p>
                            <b>EMNLP 2023 </b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b>ClusterFomer: Clustering As A Universal Visual Learner</b></br>
                            <p><i>James Liang, Yiming Cui, Qifan Wang, Tong Geng, Wenguan Wang, 
                                    <b>Dongfang Liu*</b>.</p>
                            <b>NeurIPS 2023</b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b> E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning</b></br>
                            <p><i>Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao, Wenguan Wang, Siyuan Qi, 
                                    <b>Dongfang Liu*</b>.</p>
                            <b>ICCV 2023</b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b> Tripartite Feature Enhanced Pyramid Network for Dense Prediction</b></br>
                            <p><i><b>Dongfang Liu*</b>, James Liang, Tony Geng, Alexander Loui, Tianfei Zhou.</p>
                            <b>TIP 2023</b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
             <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b> CLUSTSEG: Clustering for Universal Segmentation</b></br>
                            <p><i>James Liang, Tianfei Zhou, <b>Dongfang Liu*</b>, Wenguan Wang.</p>
                            <b>ICML 2023</b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b> MixPAVE: Mix-Prompt Tuning for Few-shot Product Attribute Value Extraction</b></br>
                            <p><i>Li Yang, Qifan Wang, Jingang Wang, Xiaojun Quan, Fuli Feng, Yu Chen, Madian Khabsa, Sinong Wang, Zenglin Xu,  
                                    <b>Dongfang Liu*</b>.</p>
                            <b>ACL 2023</b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b> MUSTIE: Multimodal Structural Transformer for Web Information Extraction</b></br>
                            <p><i>Qifan Wang, Jingang Wang, Xiaojun Quan, Fuli Feng, Zenglin Xu, Shaoliang Nie, Sinong Wang, Madian Khabsa, Hamed Firooz,  
                                    <b>Dongfang Liu*</b>.</p>
                            <b>ACL 2023</b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b> Prompt Learns Prompt: Exploring Knowledge-Aware Generative Prompt Collaboration For Video Captioning</b></br>
                            <p><i>Liqi Yan, Cheng Han, Zenglin Xu,  
                                    <b>Dongfang Liu*</b>, Qifan Wang.</p>
                            <b>IJCAI 2023</b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b> TransFlow: Transformer as Flow Learner</b></br>
                            <p><i>Yawen Lu, Qifan Wang, Siqi Ma, Tong Geng, Yingjie Victor Chen, Huaijin Chen,
                                    <b>Dongfang Liu*</b>.</p>
                            <b>CVPR 2023</b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/visual rec.png" /></div> -->
                        <div class="paper-info">
                            <b>Visual Recognition with Deep Nearest Centroids</b></br>
                            <i>Wenguan Wang, Cheng Han, Tianfei Zhou, <b>Dongfang Liu*</b>.</br>
                                <b>ICLR 2023</b></br>
                                <!-- <div class="paper-link">
                                    <a href="https://arxiv.org/abs/2209.07383">Paper</a>
                                    <a href="https://github.com/ChengHan111/DNC">Code</a>
                                </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/paper_1.png" /></div> -->
                        <div class="paper-info">
                            <b> Adversarial Training of Self-supervised Monocular Depth Estimation against
                                Physical-World Attacks</b></br>
                            <i> Zhiyuan Cheng, James Liang, Guanhong Tao, <b>Dongfang Liu*</b>, Xiangyu
                                Zhang*.</br>
                                <b>ICLR 2023</b></br>
                                <!-- <div class="paper-link">
                                    <a href="https://arxiv.org/abs/2301.13487">Paper</a>
                                    <a href="https://github.com/Bob-cheng/DepthModelHardening">Code</a>
                                </div> -->
                        </div>
                    </div>
                    <section class="paper-more" style="display: none;">
                        <div class="paper-year">2022</div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/LearningtoGenerateQuestion.png" /></div> -->
                            <div class="paper-info">
                                <b> Learning to Generate Question by Asking Question: A Primal-Dual Approach with
                                    Uncommon Word Generation</b></br>
                                <i>Qifan Wang, Li Yang, Xiaojun Quan, Fuli Feng, <b>Dongfang Liu</b>, Zenglin Xu,
                                    Sinong Wang and Hao Ma.</br>
                                    <b>EMNLP 2022</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://aclanthology.org/2022.emnlp-main.4/">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/LearningEquivariantSeg.png" /></div> -->
                            <div class="paper-info">
                                <b> Learning Equivariant Segmentation with Instance-Unique Querying</b></br>
                                <i>Wenguan Wang, James Liang, <b>Dongfang Liu*</b>.</br>
                                    <b>NeurIPS 2022</b></br>
                                    <!-- <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2210.00911">Paper</a>
                                        <a href="https://github.com/JamesLiang819/Instance_Unique_Querying">Code</a>
                                    </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/TowardsUmbiased.png" /></div> -->
                            <div class="paper-info">
                                <b> Towards Unbiased Label Distribution Learning for Facial Pose Estimation Using
                                    Anisotropic Spherical Gaussian</b></br>
                                <i>Zhiwen Cao#, <b>Dongfang Liu#</b>, Qifan Wang, and Yingjie Chen. (# first
                                    coauthor).</br>
                                    <b>ECCV 2022</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://arxiv.org/abs/2208.09122">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/PhysicalAttack.png" /></div> -->
                            <div class="paper-info">
                                <b> Physical Attack on Monocular Depth Estimation in Autonomous Driving with Optimal
                                    Adversarial Patches</b></br>
                                <i>Zhiyuan Cheng, James Liang, Hongjun Choi, Guanhong Tao, Zhiwen Cao, <b>Dongfang
                                        Liu*</b>, and Xiangyu Zhang*.</br>
                                    <b>ECCV 2022</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://arxiv.org/abs/2207.04718">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/VideoCaptioning.png" /></div> -->
                            <div class="paper-info">
                                <b> Video Captioning Using Global-Local Representation [J]</b></br>
                                <i>Liqi Yan, Siqi Ma, Qifan Wang, Yingjie Chen, Xiangyu Zhang, Andreas Savakis, and
                                    <b>Dongfang Liu*</b>.</br>
                                    <b>TCSVT 2022</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://ieeexplore.ieee.org/abstract/document/9780119">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/GL-RG.png" /></div> -->
                            <div class="paper-info">
                                <b> GL-RG: Global-Local Representation Granularity for Video Captioning</b></br>
                                <i>Liqi Yan, Qifan Wang, Yiming Cui, Fuli Feng, Xiaojun Quan, Xiangyu Zhang, and
                                    <b>Dongfang Liu*</b>.</br>
                                    <b>IJCAI 2022</b></br>
                                    <!-- <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2205.10706">Paper</a>
                                        <a href="https://github.com/ylqi/GL-RG">Code</a>
                                    </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/Deep Partial.png" /></div> -->
                            <div class="paper-info">
                                <b> Deep Partial Multiplex Network Embedding</b></br>
                                <i>Qifan Wang, Yi Fang, Anirudh Ravula, Ruining He, Bin Shen, Jingang Wang, Xiaojun
                                    Quan, and <b>Dongfang Liu*</b>.</br>
                                    <b>WWW 2022</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://dl.acm.org/doi/abs/10.1145/3487553.3524717">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/WebFormer.png" /></div> -->
                            <div class="paper-info">
                                <b> WebFormer: The Web-page Transformer for Structure Information
                                    Extraction</b></br>
                                <i>Qifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiaojun Quan, and <b>Dongfang
                                        Liu*</b>.</br>
                                    <b>WWW 2022</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://arxiv.org/abs/2202.00217">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/DGL_MOST.png" /></div> -->
                            <div class="paper-info">
                                <b> DG-Labeler and DGL-MOTS Dataset: Boost the Autonomous Driving
                                    Perception</b></br>
                                <i>Yiming Cui, Zhiwen Cao, Yixin Xie, Xingyu Jiang, Feng Tao, Victor Chen, Lin Li,
                                    and <b>Dongfang Liu*</b>.</br>
                                    <b>WACV 2022</b></br>
                                    <!-- <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2110.07790">Paper</a>
                                        <a href="https://goodproj13.github.io/DGL-MOTS/">Code</a>
                                    </div> -->
                            </div>
                        </div>
                        <div class="paper-year">2021</div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/TF-Blender.png" /></div> -->
                            <div class="paper-info">
                                <b> TF-Blender: Temporal Feature Blender for Video Object Detection</b></br>
                                <i>Yiming Cui, Liqi Yan, Zhiwen Cao, and <b>Dongfang Liu*</b>.</br>
                                    <b>ICCV 2021</b></br>
                                    <!-- <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2108.05821">Paper</a>
                                        <a href="https://github.com/goodproj13/TF-Blender">Code</a>
                                    </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/SG-Net.png" /></div> -->
                            <div class="paper-info">
                                <b> SG-Net: Spatial Granularity Network for One-Stage Video Instance Segmentation</b></br>
                                <i><b>Dongfang Liu</b>, Yiming Cui, and Yingjie Chen.</br>
                                    <b>CVPR 2021</b></br>
                                    <!-- <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2103.10284">Paper</a>
                                        <a href="https://github.com/goodproj13/SG-Net">Code</a>
                                    </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/DenserNet.png" /></div> -->
                            <div class="paper-info">
                                <b> DenserNet: Weakly Supervised Visual Localization Using Multi-scale
                                    Features</b></br>
                                <i><b>Dongfang Liu</b>, Yiming Cui, Baijian Yang, and Yingjie Chen.</br>
                                    <b>AAAI 2021</b></br>
                                    <!-- <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2012.02366">Paper</a>
                                        <a href="https://github.com/goodproj13/DenserNet">Code</a>
                                    </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/Vectorbased.png" /></div> -->
                            <div class="paper-info">
                                <b> A Vector-Based Representation to Enhance Head Pose Estimation</b></br>
                                <i>Zhiwen Cao, Zongcheng Chu, <b>Dongfang Liu</b>, and Yingjie Chen.</br>
                                    <b>WACV 2021</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://arxiv.org/abs/2010.07184">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/Hierarchical.png" /></div> -->
                            <div class="paper-info">
                                <b> Hierarchical Attention Fusion for Geo-Localization</b></br>
                                <i>Liqi Yan, Yiming Cui, Yingjie Chen, and <b>Dongfang Liu</b>.</br>
                                    <b>ICASSP 2021</b></br>
                                    <!-- <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2102.09186">Paper</a>
                                        <a href="https://github.com/ylqi/HAF">Code</a>
                                    </div> -->
                            </div>
                        </div>
                        <div class="paper-year">2020</div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/MultimodalAgg.png" /></div> -->
                            <div class="paper-info">
                                <b> Multimodal Aggregation Approach for Memory Vision-Voice Indoor Navigation with
                                    Meta-Learning</b></br>
                                <i>Liqi Yan#, <b>Dongfang Liu#</b>, and Changbin Yu. (# first coauthor).</br>
                                    <b>IROS 2020</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://arxiv.org/abs/2009.00402">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                    </section>
                </div>
                </div>
                <hr>

                <div class="row" id="footer">
                    <div class="col-md-12 text-center">
                        <script type='text/javascript' id='clustrmaps'
                            src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=n&d=hzX_uL3VVw4MumSglN7Q5_qHn2xgfao5PAYGLiieOCY'></script>
                        <!--<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=ubcKPhBPgXUfkNsB2U63LQjLkn3-FjAMmvrqlRrU9WU&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>-->
                        <p>The template of this website is designed by <a
                                href="http://www.templatemo.com/live/templatemo_441_volton">templatemo</a>.</p>
                        <!--<p>The template of this website is designed by <a href="https://clustrmaps.com/profile/1bfmz/widget/code/globe">templatemo</a>.</p>-->
                    </div>
                </div>

            </div>

        </div>
    </div>

    <script src="js/vendor/jquery-1.10.2.min.js"></script>
    <script src="js/min/plugins.min.js"></script>
    <script src="js/min/main.min.js"></script>
    <script type='text/javascript'>
        var readMorePapers = document.getElementById('readMore'),
            readMoreProjects = document.getElementById('readMoreProjects')
        function moreClick(ele, type) {
            var contentSection = document.getElementById(type),
                list = contentSection.getElementsByClassName('paper-more')[0]
            list.style.display = list.style.display === 'block' ? 'none' : 'block'
            var arrow = ele.getElementsByTagName('i')[0]
            console.log(arrow, 'arrowarrow')
            arrow.className = arrow.className === 'down' ? 'up' : 'down'
        }
        function publicationsClickEvent() {
            moreClick(readMorePapers, 'publications')
        }
        function projectsClickEvent() {
            moreClick(readMoreProjects, 'projects')
        }
        function bindEvent() {
            readMorePapers.addEventListener('click', publicationsClickEvent, false)
            readMoreProjects.addEventListener('click', projectsClickEvent, false)
        }
        bindEvent()
    </script>
    <script type="text/javascript" src="components/tab/tab.js"></script>
    <script type="text/javascript">
        var tab = new Tab({
            tabItem :'tab-item',
            pageItem: 'page-item',
            cur: 'cur',
            active: 'active'
        })
    </script>
</body>

</html>


