<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js">
<!--<![endif]-->

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Dongfang Liu's website</title>
    <meta name="description" content="UTSA">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="css/normalize.css">
    <link rel="stylesheet" href="css/font-awesome.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/templatemo-style.css">
    <script src="js/vendor/modernizr-2.6.2.min.js"></script>
</head>

<body>
    <!--[if lt IE 7]>
            <p class="browsehappy">You are using an <strong>outdated</strong> browser. Please upgrade your browser to improve your experience.</p>
        <![endif]-->

    <div class="responsive-header visible-xs visible-sm">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <div class="top-section">
                        <div class="profile-image">
                            <img src="img/dongfang.jpg" backgroundSize="contain" alt="Dongfang Liu">
                        </div>
                        <div class="profile-content">
                            <h3 class="profile-title">Dongfang Liu</h3>
                            <p class="profile-description">Rochester Institute of Technology</p>
                            <p class="profile-description">dongfang.liu AT rit.edu</p>
                        </div>
                    </div>
                </div>
            </div>
            <a href="#" class="toggle-menu"><i class="fa fa-bars"></i></a>
            <div class="main-navigation responsive-menu">
                <ul class="navigation">
                    <li><a href="#top"><i class="fa fa-home"></i>Home</a></li>
                    <li><a href="#about"><i class="fa fa-user"></i>About Me</a></li>
                    <li><a href="#research"><i class="fa fa-newspaper-o"></i>Research</a></li>
                    <li><a href="#publications"><i class="fa fa-book"></i>Publications</a></li>
                </ul>
            </div>
        </div>
    </div>

    <!-- SIDEBAR -->
    <div class="sidebar-menu hidden-xs hidden-sm">
        <div class="top-section">
            <div class="profile-image">
                <img src="img/dongfang.jpg" alt="Dongfang Liu">
            </div>
            <h3 class="profile-title">Dr. Dongfang Liu</h3>
            <p class="profile-description">Assistant professor @ RIT</p>
            <p class="profile-description">Ph.D from Purdue University (2021)</p>
            <p class="profile-description">dongfang.liu AT rit.edu</p>
        </div> <!-- top-section -->
        <div class="main-navigation">
            <ul class="navigation">
                <li><a href="#top"><i class="fa fa-home"></i>Home</a></li>
                <li><a href="#about"><i class="fa fa-user"></i>About Me</a></li>
                <li><a href="#interests"><i class="fa fa-newspaper-o"></i>Research Interests</a></li>
                <li><a href="#projects"><i class="fa fa-file-o"></i>Research Projects</a></li>
                <li><a href="#services"><i class="fa fa-cogs"></i>Services</a></li>
                <li><a href="#publications"><i class="fa fa-book"></i>Publications</a></li>
            </ul>
        </div> <!-- .main-navigation -->
        <!--<div class="social-icons">-->
        <!--<ul>-->
        <!--<li><a href="#"><i class="fa fa-facebook"></i></a></li>-->
        <!--<li><a href="#"><i class="fa fa-linkedin"></i></a></li>-->
        <!--<li><a href="https://scholar.google.com/citations?user=_Voig00AAAAJ&hl=en" target="_blank"><i class="fa fa-google"></i></a></li>-->
        <!--<li><a href="#"><i class="fa fa-rss"></i></a></li>-->
        <!--</ul>-->
        <!--</div> &lt;!&ndash; .social-icons &ndash;&gt;-->
    </div> <!-- .sidebar-menu -->

    <div class="banner-bg" id="top">
        <!--<div class="banner-overlay"></div>-->
        <!-- <div class="welcome-text">
                <h2>Boiler up</h2>
            </div> -->
    </div>

    <!-- MAIN CONTENT -->
    <div class="main-content">
        <div class="fluid-container">

            <div class="content-wrapper">

                <!-- ABOUT -->
                <div class="page-section" id="about">
                    <div class="row">
                        <div class="col-md-12">
                            <h4 class="widget-title">About Me</h4>
                            <p> I am an Assistant Professor in the Department of Computer Engineering at the Rochester
                                Institute of Technology (RIT). My research interests include AI/ML/DL/CV/HCI.
                                I have a handsome boy üêà Mr. "Tiger", who is apparently well-known at RIT.
                                <hr>
                        </div>
                    </div> <!-- #about -->
                </div>

                <!-- Research -->
                <div class="page-section" id="interests">
                    <h4 class="widget-title">Research Interests</h4>
                    <div class="row">
                        <div class="col-md-12">
                            <div class="project-item">
                                <!--<b>Record and Replay</b>-->
                                <!--<p>
                        Reproducing errors of multithreaded programs is very challenging due to many intrinsic and external non-deterministic factors. Existing RnR systems achieve significant progress in terms of performance overhead, but none target the in-situ setting, in which replay occurs within the same process as the recording process. Also, most cannot achieve identical replay, which may prevent the reproduction of some errors.
                        </p>-->
                                <p>
                                    I have dedicated my research efforts to develop general AI solutions for
                                    interdisciplinary research to address challenges with societal relevance. My studied
                                    domains span from transportation to agriculture, and to medical science. I am
                                    continuously looking for <b>
                                        <font color="#ff4500"> doctoral/master/undergrad students</font>
                                    </b> to work with my research projects. If you want to develop awesome AI tools for
                                    real-world problems, I think we need to talk üòÑ
                                </p>
                            </div>
                        </div>
                    </div>
                </div> <!-- .research-holder -->
                <hr>


                <!-- Research Projects-->
                <div class="page-section" id="projects">
                    <h4 class="widget-title">
                        Research Projects
                        <p id="readMoreProjects"><i class="down"></i></p>
                    </h4>
                    <div class="project-container">
                        <div class="project-item top">
                            <!-- <div class="project-image"><img src="img/visual rec.png" /></div> -->
                            <div class="tab-wrap">
                                <div class="tab clearfix">
                                    <div class="tab-item cur">
                                        <a href="javascript:;"></a>
                                    </div>
                                    <div class="tab-item">
                                        <a href="javascript:;"></a>
                                    </div>
                                    <div class="tab-item">
                                        <a href="javascript:;"></a>
                                    </div>
                                </div>
                                <div class="page">
                                    <div class="page-item active">
                                        <img src="img/transflow.png" />
                                    </div>
                                    <div class="page-item">
                                        <img src="img/visual rec.png" />
                                    </div>
                                    <div class="page-item">
                                        <img src="img/Selfadversial.png" />
                                    </div>
                                </div>
                            </div>
                            <div class="project-info">
                                <b><a href="./projects/project1.html"><b>Project 1</b>: Vision-Anchored Automation of
                                    Bird-Sized UAVs in Unknown Cluttered Indoor Environments</a></b></br>
                                <i>The research project aims to develop full automation for bird-sized UAVs using only an RGB-D camera, and seeks to answer fundamental questions for UAV automation, including constructing visual perception and developing visual navigation.</br>
                            </div>
                        </div>
                        <img class="nsf" src="./img/nsf.png">
                    </div>
                    <section class="paper-more" style="display: none;">
                        <div class="project-container">
                            <div class="project-item">
                                <!-- <div class="project-image"><img src="img/LearningtoGenerateQuestion.png" /></div>
                                <div class="project-info">
                                    <b><a href="./projects/project2.html"><b>Project 2</b>:  Vision-Anchored Automation of
                                        Bird-Sized UAVs in Unknown Cluttered Indoor Environments</a></b>
                                </div> -->
                                <!-- <div class="tab-wrap">
                                    <div class="tab clearfix">
                                        <div class="tab-item cur">
                                            <a href="javascript:;"></a>
                                        </div>
                                        <div class="tab-item">
                                            <a href="javascript:;"></a>
                                        </div>
                                        <div class="tab-item">
                                            <a href="javascript:;"></a>
                                        </div>
                                    </div>
                                    <div class="page">
                                        <div class="page-item active">
                                            <img src="img/transflow.png" />
                                        </div>
                                        <div class="page-item">
                                            <img src="img/visual rec.png" />
                                        </div>
                                        <div class="page-item">
                                            <img src="img/Selfadversial.png" />
                                        </div>
                                    </div>
                                </div>
                                <div class="project-info">
                                    <b><a href="./projects/project2.html"><b>Project 2</b>: Vision-Anchored Automation of
                                        Bird-Sized UAVs in Unknown Cluttered Indoor Environments</a></b></br>
                                    <i>The research project aims to develop full automation for bird-sized UAVs using only an RGB-D camera, and seeks to answer fundamental questions for UAV automation, including constructing visual perception and developing visual navigation.</br>
                                </div>
                            </div>
                            <img class="nsf" src="./img/nsf.png"> -->
                        </div>
                    </section>
                </div>
                <hr>

                <!-- Service -->
                <div class="page-section" id="services">
                    <h4 class="widget-title">Services</h4>
                    <div class="row">
                        <div class="col-md-12">
                            <div class="project-item">
                                <ul>
                                    <li><b>Associate editor</b>: IEEE Transactions on Circuits and Systems for Video
                                        Technology (TCSVT), 2023-</li>
                                    <li><b>Senior PC</b>: The International Joint Conference on Artificial
                                        Intelligence (IJCAI), 2023
                                    <li><b>Senior PC</b>: The Association for the Advancement of Artificial
                                        Intelligence (AAAI), 2023
                                    <li><b>Reviewer</b>: The Conference on Computer Vision and Pattern Recognition
                                        (CVPR), 2020-</li>
                                    <li><b>Reviewer</b>: The European Conference on Computer Vision (ECCV), 2020-
                                    </li>
                                    <li><b>Reviewer</b>: The International Conference on Computer Vision (ICCV),
                                        2021-</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
                <hr>




                <!-- publications -->
                <!-- <div class="page-section" id="publications">
                        <h4 class="widget-title">Recent Publications</h4>
                        <div class="row">
                          <div class="col-md-12">
                            <dl class="dl-horizontal">
                            <dt>IJCAI 2023</dt>
                              <dd>
                                <b>	Prompt Learns Prompt: Exploring Knowledge-Aware Generative Prompt Collaboration For Video Captioning</b></br>
                                <i>Liqi Yan, Cheng Han, Zenglin Xu, Dongfang Liu*, Qifan Wang.
                              </dd>
                            <dt>CVPR 2023</dt>
                              <dd>
                                <b>	TransFlow: Transformer as Flow Learner</b></br>
                                <i>Yawen Lu, Qifan Wang, Siqi Ma, Tong Geng, Yingjie Victor Chen, Huaijin Chen, Dongfang Liu*.
                              </dd>
                              <dt>ICLR 2023</dt>
                              <dd>
                                <b>	Visual Recognition with Deep Nearest Centroids</b></br>
                                <i>Wenguan Wang, Cheng Han, Tianfei Zhou, Dongfang Liu*.
                              </dd>
                                 <dt>ICLR 2023</dt>
                              <dd>
                                <b>Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks</b></br>
                                <i>Zhiyuan Cheng, James Chenhao Liang, Guanhong Tao, Dongfang Liu*, Xiangyu Zhang*.
                              </dd>
                             <dt>EMNLP 2022</dt>
                              <dd>
                                <b>Learning to Generate Question by Asking Question: A Primal-Dual Approach with Uncommon Word Generation</b></br>
                                <i>Qifan Wang, Li Yang, Xiaojun Quan, Fuli Feng, Dongfang Liu, Zenglin Xu, Sinong Wang and Hao Ma.
                              </dd>
                                <dt>NIPS 2022</dt>
                              <dd>
                                <b>Learning Equivariant Segmentation with Instance-Unique Querying</b></br>
                                <i> Wenguan Wang, James Liang, Dongfang Liu*.
                              </dd>
                              <dt>ECCV 2022</dt>
                              <dd>
                                <b>Towards Unbiased Label Distribution Learning for Facial Pose Estimation Using Anisotropic Spherical Gaussian</b></br>
                                <i> Zhiwen Cao#, Dongfang Liu#, Qifan Wang, and Yingjie Chen. </i> (# first coauthor) </br>
                              </dd>
                              <dt>ECCV 2022</dt>
                              <dd>
                                <b>Physical Attack on Monocular Depth Estimation in Autonomous Driving with Optimal Adversarial Patches</b></br>
                                <i> Zhiyuan Cheng, James Liang, Hongjun Choi, Guanhong Tao, Zhiwen Cao, Dongfang Liu*, and Xiangyu Zhang*.
                              </dd>
                            <dt>TCSVT 2022</dt>
                              <dd>
                                <b>Video Captioning Using Global-Local Representation [J]</b></br>
                                <i>Liqi Yan, Siqi Ma, Qifan Wang, Yingjie Chen, Xiangyu Zhang, Andreas Savakis, and Dongfang Liu*.
                              </dd>
                                 <dt>IJCAI 2022</dt>
                              <dd>
                                <b>GL-RG: Global-Local Representation Granularity for Video Captioning</b></br>
                                <i>Liqi Yan, Qifan Wang, Yiming Cui, Fuli Feng, Xiaojun Quan, Xiangyu Zhang, and Dongfang Liu*.
                              </dd>
                                <dt>WWW 2022</dt>
                              <dd>
                                <b>Deep Partial Multiplex Network Embedding</b></br>
                                <i>Qifan Wang, Yi Fang, Anirudh Ravula, Ruining He, Bin Shen, Jingang Wang, Xiaojun Quan, and Dongfang Liu*.
                                <dt>WWW 2022</dt>
                              <dd>
                                <b>WebFormer: The Web-page Transformer for Structure Information Extraction</b></br>
                                <i>Qifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiaojun Quan, and Dongfang Liu*.
                              </dd>
                                <dt>WACV 2022</dt>
                              <dd>
                                <b>DG-Labeler and DGL-MOTS Dataset: Boost the Autonomous Driving Perception</b></br>
                                <i>Yiming Cui, Zhiwen Cao, Yixin Xie, Xingyu Jiang, Feng Tao, Victor Chen, Lin Li, and Dongfang Liu*.
                              </dd>
                                <dt>ICCV 2021</dt>
                              <dd>
                                <b>TF-Blender: Temporal Feature Blender for Video Object Detection</b></br>
                                <i>Yiming Cui, Liqi Yan, Zhiwen Cao and Dongfang Liu*.
                              </dd>
                                <dt>CVPR 2021</dt>
                              <dd>
                                <b>SG-Net: Spatial Granularity Network for Multi-Object Tracking and Segmentation</b></br>
                                <i>Dongfang Liu, Yiming Cui, and Yingjie Chen.</i> 
                              </dd>
                              <dt>AAAI 2021</dt>
                              <dd>
                                <b>DenserNet:  Weakly  Supervised Visual Localization Using Multi-scale Features</b></br>
                                <i>Dongfang Liu, Yiming Cui, Baijian Yang, and Yingjie Chen.</i> 
                              </dd>
                                <dt>WACV 2021</dt>
                              <dd>
                                <b>A Vector-Based Representation to Enhance Head Pose Estimation</b> </br>
                                <i>Zhiwen Cao, Zongcheng Chu, Dongfang Liu, and Yingjie Chen.</i>
                              </dd>
                                <dt>ICASSP 2021</dt>
                              <dd>
                                <b>Hierarchical Attention Fusion for Geo-Localization</b> </br>
                                <i>Liqi Yan, Yiming Cui, Yingjie Chen, and Dongfang Liu*. 
                              </dd>
                              <dt>IROS 2020</dt>
                              <dd>
                                <b>Multimodal Aggregation Approachfor Memory Vision-Voice Indoor Navigation with Meta-Learning</b></br>
                                <i>Liqi Yan#, Dongfang Liu#, and Changbin Yu.</i> (# first coauthor) </br>
                               </dd>
                            </dl>
                          </div>
                        </div>
                        </div> -->
                <!--.publications-holder -->
                <!-- <hr> -->

                <!-- publications old-->
                <!-- <div class="page-section" id="publications">
                    <h4 class="widget-title">Publications<p id="readMore"><i class="down"></i></p>
                    </h4>
                    <div class="paper-item top">
                        <div class="paper-image"><img src="img/transflow.png" /></div>
                        <div class="paper-info">
                            <b> TransFlow: Transformer as Flow Learner</b></br>
                            <p><i>Yawen Lu, Qifan Wang, Siqi Ma, Tong Geng, Yingjie Victor Chen, Huaijin Chen,
                                    <b>Dongfang Liu*</b>.</p>
                            <b>CVPR 2023</b></br>
                            <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div>
                        </div>
                    </div>
                    <div class="paper-item top">
                        <div class="paper-image"><img src="img/visual rec.png" /></div>
                        <div class="paper-info">
                            <b>Visual Recognition with Deep Nearest Centroids</b></br>
                            <i>Wenguan Wang, Cheng Han, Tianfei Zhou, <b>Dongfang Liu*</b>.</br>
                                <b>ICLR 2023</b></br>
                                <div class="paper-link">
                                    <a href="https://arxiv.org/abs/2209.07383">Paper</a>
                                    <a href="https://github.com/ChengHan111/DNC">Code</a>
                                </div>
                        </div>
                    </div>
                    <div class="paper-item top">
                        <div class="paper-image"><img src="img/paper_1.png" /></div>
                        <div class="paper-info">
                            <b> Adversarial Training of Self-supervised Monocular Depth Estimation against
                                Physical-World Attacks</b></br>
                            <i> Zhiyuan Cheng, James Chenhao Liang, Guanhong Tao, <b>Dongfang Liu*</b>, Xiangyu
                                Zhang*.</br>
                                <b>ICLR 2023</b></br>
                                <div class="paper-link">
                                    <a href="https://arxiv.org/abs/2301.13487">Paper</a>
                                    <a href="https://github.com/Bob-cheng/DepthModelHardening">Code</a>
                                </div>
                        </div>
                    </div>
                    <section class="paper-more" style="display: none;">
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/LearningtoGenerateQuestion.png" /></div>
                            <div class="paper-info">
                                <b> Learning to Generate Question by Asking Question: A Primal-Dual Approach with
                                    Uncommon Word Generation</b></br>
                                <i>Qifan Wang, Li Yang, Xiaojun Quan, Fuli Feng, <b>Dongfang Liu</b>, Zenglin Xu,
                                    Sinong Wang and Hao Ma.</br>
                                    <b>EMNLP 2022</b></br>
                                    <div class="paper-link">
                                        <a href="https://aclanthology.org/2022.emnlp-main.4/">Paper</a>
                                    </div>
                            </div>
                        </div>
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/LearningEquivariantSeg.png" /></div>
                            <div class="paper-info">
                                <b> Learning Equivariant Segmentation with Instance-Unique Querying</b></br>
                                <i>Wenguan Wang, James Liang, <b>Dongfang Liu*</b>.</br>
                                    <b>NIPS 2022</b></br>
                                    <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2210.00911">Paper</a>
                                        <a href="https://github.com/JamesLiang819/Instance_Unique_Querying">Code</a>
                                    </div>
                            </div>
                        </div>
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/TowardsUmbiased.png" /></div>
                            <div class="paper-info">
                                <b> Towards Unbiased Label Distribution Learning for Facial Pose Estimation Using
                                    Anisotropic Spherical Gaussian</b></br>
                                <i>Zhiwen Cao#, <b>Dongfang Liu#</b>, Qifan Wang, and Yingjie Chen. (# first
                                    coauthor).</br>
                                    <b>ECCV 2022</b></br>
                                    <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2208.09122">Paper</a>
                                    </div>
                            </div>
                        </div>
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/PhysicalAttack.png" /></div>
                            <div class="paper-info">
                                <b> Physical Attack on Monocular Depth Estimation in Autonomous Driving with Optimal
                                    Adversarial Patches</b></br>
                                <i>Zhiyuan Cheng, James Liang, Hongjun Choi, Guanhong Tao, Zhiwen Cao, <b>Dongfang
                                        Liu*</b>, and Xiangyu Zhang*.</br>
                                    <b>ECCV 2022</b></br>
                                    <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2207.04718">Paper</a>
                                    </div>
                            </div>
                        </div>
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/VideoCaptioning.png" /></div>
                            <div class="paper-info">
                                <b> Video Captioning Using Global-Local Representation [J]</b></br>
                                <i>Liqi Yan, Siqi Ma, Qifan Wang, Yingjie Chen, Xiangyu Zhang, Andreas Savakis, and
                                    <b>Dongfang Liu*</b>.</br>
                                    <b>TCSVT 2022</b></br>
                                    <div class="paper-link">
                                        <a href="https://ieeexplore.ieee.org/abstract/document/9780119">Paper</a>
                                    </div>
                            </div>
                        </div>
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/GL-RG.png" /></div>
                            <div class="paper-info">
                                <b> GL-RG: Global-Local Representation Granularity for Video Captioning</b></br>
                                <i>Liqi Yan, Qifan Wang, Yiming Cui, Fuli Feng, Xiaojun Quan, Xiangyu Zhang, and
                                    <b>Dongfang Liu*</b>.</br>
                                    <b>IJCAI 2022</b></br>
                                    <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2205.10706">Paper</a>
                                        <a href="https://github.com/ylqi/GL-RG">Code</a>
                                    </div>
                            </div>
                        </div>
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/Deep Partial.png" /></div>
                            <div class="paper-info">
                                <b> Deep Partial Multiplex Network Embedding</b></br>
                                <i>Qifan Wang, Yi Fang, Anirudh Ravula, Ruining He, Bin Shen, Jingang Wang, Xiaojun
                                    Quan, and <b>Dongfang Liu*</b>.</br>
                                    <b>WWW 2022</b></br>
                                    <div class="paper-link">
                                        <a href="https://dl.acm.org/doi/abs/10.1145/3487553.3524717">Paper</a>
                                    </div>
                            </div>
                        </div>
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/WebFormer.png" /></div>
                            <div class="paper-info">
                                <b> WebFormer: The Web-page Transformer for Structure Information
                                    Extraction</b></br>
                                <i>Qifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiaojun Quan, and <b>Dongfang
                                        Liu*</b>.</br>
                                    <b>WWW 2022</b></br>
                                    <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2202.00217">Paper</a>
                                    </div>
                            </div>
                        </div>
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/DGL_MOST.png" /></div>
                            <div class="paper-info">
                                <b> DG-Labeler and DGL-MOTS Dataset: Boost the Autonomous Driving
                                    Perception</b></br>
                                <i>Yiming Cui, Zhiwen Cao, Yixin Xie, Xingyu Jiang, Feng Tao, Victor Chen, Lin Li,
                                    and <b>Dongfang Liu*</b>.</br>
                                    <b>WACV 2022</b></br>
                                    <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2110.07790">Paper</a>
                                        <a href="https://goodproj13.github.io/DGL-MOTS/">Code</a>
                                    </div>
                            </div>
                        </div>
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/TF-Blender.png" /></div>
                            <div class="paper-info">
                                <b> TF-Blender: Temporal Feature Blender for Video Object Detection</b></br>
                                <i>Yiming Cui, Liqi Yan, Zhiwen Cao, and <b>Dongfang Liu*</b>.</br>
                                    <b>ICCV 2021</b></br>
                                    <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2108.05821">Paper</a>
                                        <a href="https://github.com/goodproj13/TF-Blender">Code</a>
                                    </div>
                            </div>
                        </div>
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/SG-Net.png" /></div>
                            <div class="paper-info">
                                <b> SG-Net: Spatial Granularity Network for Multi-Object Tracking and
                                    Segmentation</b></br>
                                <i><b>Dongfang Liu</b>, Yiming Cui, and Yingjie Chen.</br>
                                    <b>CVPR 2021</b></br>
                                    <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2103.10284">Paper</a>
                                        <a href="https://github.com/goodproj13/SG-Net">Code</a>
                                    </div>
                            </div>
                        </div>
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/DenserNet.png" /></div>
                            <div class="paper-info">
                                <b> DenserNet: Weakly Supervised Visual Localization Using Multi-scale
                                    Features</b></br>
                                <i><b>Dongfang Liu</b>, Yiming Cui, Baijian Yang, and Yingjie Chen.</br>
                                    <b>AAAI 2021</b></br>
                                    <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2012.02366">Paper</a>
                                        <a href="https://github.com/goodproj13/DenserNet">Code</a>
                                    </div>
                            </div>
                        </div>
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/Vectorbased.png" /></div>
                            <div class="paper-info">
                                <b> A Vector-Based Representation to Enhance Head Pose Estimation</b></br>
                                <i>Zhiwen Cao, Zongcheng Chu, <b>Dongfang Liu</b>, and Yingjie Chen.</br>
                                    <b>WACV 2021</b></br>
                                    <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2010.07184">Paper</a>
                                    </div>
                            </div>
                        </div>
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/Hierarchical.png" /></div>
                            <div class="paper-info">
                                <b> Hierarchical Attention Fusion for Geo-Localization</b></br>
                                <i>Liqi Yan, Yiming Cui, Yingjie Chen, and <b>Dongfang Liu</b>.</br>
                                    <b>ICASSP 2021</b></br>
                                    <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2102.09186">Paper</a>
                                        <a href="https://github.com/ylqi/HAF">Code</a>
                                    </div>
                            </div>
                        </div>
                        <div class="paper-item">
                            <div class="paper-image"><img src="img/MultimodalAgg.png" /></div>
                            <div class="paper-info">
                                <b> Multimodal Aggregation Approachfor Memory Vision-Voice Indoor Navigation with
                                    Meta-Learning</b></br>
                                <i>Liqi Yan#, <b>Dongfang Liu#</b>, and Changbin Yu. (# first coauthor).</br>
                                    <b>IROS 2020</b></br>
                                    <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2009.00402">Paper</a>
                                    </div>
                            </div>
                        </div>
                    </section>
                </div>
                </div>
                <hr> -->


                <!-- publications new-->
                <div class="page-section" id="publications">
                    <h4 class="widget-title">Publications<p id="readMore"><i class="down"></i></p>
                    </h4>
                    <div class="paper-year">2023</div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/transflow.png" /></div> -->
                        <div class="paper-info">
                            <b> TransFlow: Transformer as Flow Learner</b></br>
                            <p><i>Yawen Lu, Qifan Wang, Siqi Ma, Tong Geng, Yingjie Victor Chen, Huaijin Chen,
                                    <b>Dongfang Liu*</b>.</p>
                            <b>CVPR 2023</b></br>
                            <!-- <div class="paper-link">
                                <a href="">Paper</a>
                                <a href="">Code</a>
                            </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/visual rec.png" /></div> -->
                        <div class="paper-info">
                            <b>Visual Recognition with Deep Nearest Centroids</b></br>
                            <i>Wenguan Wang, Cheng Han, Tianfei Zhou, <b>Dongfang Liu*</b>.</br>
                                <b>ICLR 2023</b></br>
                                <!-- <div class="paper-link">
                                    <a href="https://arxiv.org/abs/2209.07383">Paper</a>
                                    <a href="https://github.com/ChengHan111/DNC">Code</a>
                                </div> -->
                        </div>
                    </div>
                    <div class="paper-item top">
                        <!-- <div class="paper-image"><img src="img/paper_1.png" /></div> -->
                        <div class="paper-info">
                            <b> Adversarial Training of Self-supervised Monocular Depth Estimation against
                                Physical-World Attacks</b></br>
                            <i> Zhiyuan Cheng, James Chenhao Liang, Guanhong Tao, <b>Dongfang Liu*</b>, Xiangyu
                                Zhang*.</br>
                                <b>ICLR 2023</b></br>
                                <!-- <div class="paper-link">
                                    <a href="https://arxiv.org/abs/2301.13487">Paper</a>
                                    <a href="https://github.com/Bob-cheng/DepthModelHardening">Code</a>
                                </div> -->
                        </div>
                    </div>
                    <section class="paper-more" style="display: none;">
                        <div class="paper-year">2022</div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/LearningtoGenerateQuestion.png" /></div> -->
                            <div class="paper-info">
                                <b> Learning to Generate Question by Asking Question: A Primal-Dual Approach with
                                    Uncommon Word Generation</b></br>
                                <i>Qifan Wang, Li Yang, Xiaojun Quan, Fuli Feng, <b>Dongfang Liu</b>, Zenglin Xu,
                                    Sinong Wang and Hao Ma.</br>
                                    <b>EMNLP 2022</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://aclanthology.org/2022.emnlp-main.4/">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/LearningEquivariantSeg.png" /></div> -->
                            <div class="paper-info">
                                <b> Learning Equivariant Segmentation with Instance-Unique Querying</b></br>
                                <i>Wenguan Wang, James Liang, <b>Dongfang Liu*</b>.</br>
                                    <b>NIPS 2022</b></br>
                                    <!-- <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2210.00911">Paper</a>
                                        <a href="https://github.com/JamesLiang819/Instance_Unique_Querying">Code</a>
                                    </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/TowardsUmbiased.png" /></div> -->
                            <div class="paper-info">
                                <b> Towards Unbiased Label Distribution Learning for Facial Pose Estimation Using
                                    Anisotropic Spherical Gaussian</b></br>
                                <i>Zhiwen Cao#, <b>Dongfang Liu#</b>, Qifan Wang, and Yingjie Chen. (# first
                                    coauthor).</br>
                                    <b>ECCV 2022</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://arxiv.org/abs/2208.09122">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/PhysicalAttack.png" /></div> -->
                            <div class="paper-info">
                                <b> Physical Attack on Monocular Depth Estimation in Autonomous Driving with Optimal
                                    Adversarial Patches</b></br>
                                <i>Zhiyuan Cheng, James Liang, Hongjun Choi, Guanhong Tao, Zhiwen Cao, <b>Dongfang
                                        Liu*</b>, and Xiangyu Zhang*.</br>
                                    <b>ECCV 2022</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://arxiv.org/abs/2207.04718">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/VideoCaptioning.png" /></div> -->
                            <div class="paper-info">
                                <b> Video Captioning Using Global-Local Representation [J]</b></br>
                                <i>Liqi Yan, Siqi Ma, Qifan Wang, Yingjie Chen, Xiangyu Zhang, Andreas Savakis, and
                                    <b>Dongfang Liu*</b>.</br>
                                    <b>TCSVT 2022</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://ieeexplore.ieee.org/abstract/document/9780119">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/GL-RG.png" /></div> -->
                            <div class="paper-info">
                                <b> GL-RG: Global-Local Representation Granularity for Video Captioning</b></br>
                                <i>Liqi Yan, Qifan Wang, Yiming Cui, Fuli Feng, Xiaojun Quan, Xiangyu Zhang, and
                                    <b>Dongfang Liu*</b>.</br>
                                    <b>IJCAI 2022</b></br>
                                    <!-- <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2205.10706">Paper</a>
                                        <a href="https://github.com/ylqi/GL-RG">Code</a>
                                    </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/Deep Partial.png" /></div> -->
                            <div class="paper-info">
                                <b> Deep Partial Multiplex Network Embedding</b></br>
                                <i>Qifan Wang, Yi Fang, Anirudh Ravula, Ruining He, Bin Shen, Jingang Wang, Xiaojun
                                    Quan, and <b>Dongfang Liu*</b>.</br>
                                    <b>WWW 2022</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://dl.acm.org/doi/abs/10.1145/3487553.3524717">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/WebFormer.png" /></div> -->
                            <div class="paper-info">
                                <b> WebFormer: The Web-page Transformer for Structure Information
                                    Extraction</b></br>
                                <i>Qifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiaojun Quan, and <b>Dongfang
                                        Liu*</b>.</br>
                                    <b>WWW 2022</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://arxiv.org/abs/2202.00217">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/DGL_MOST.png" /></div> -->
                            <div class="paper-info">
                                <b> DG-Labeler and DGL-MOTS Dataset: Boost the Autonomous Driving
                                    Perception</b></br>
                                <i>Yiming Cui, Zhiwen Cao, Yixin Xie, Xingyu Jiang, Feng Tao, Victor Chen, Lin Li,
                                    and <b>Dongfang Liu*</b>.</br>
                                    <b>WACV 2022</b></br>
                                    <!-- <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2110.07790">Paper</a>
                                        <a href="https://goodproj13.github.io/DGL-MOTS/">Code</a>
                                    </div> -->
                            </div>
                        </div>
                        <div class="paper-year">2021</div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/TF-Blender.png" /></div> -->
                            <div class="paper-info">
                                <b> TF-Blender: Temporal Feature Blender for Video Object Detection</b></br>
                                <i>Yiming Cui, Liqi Yan, Zhiwen Cao, and <b>Dongfang Liu*</b>.</br>
                                    <b>ICCV 2021</b></br>
                                    <!-- <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2108.05821">Paper</a>
                                        <a href="https://github.com/goodproj13/TF-Blender">Code</a>
                                    </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/SG-Net.png" /></div> -->
                            <div class="paper-info">
                                <b> SG-Net: Spatial Granularity Network for Multi-Object Tracking and
                                    Segmentation</b></br>
                                <i><b>Dongfang Liu</b>, Yiming Cui, and Yingjie Chen.</br>
                                    <b>CVPR 2021</b></br>
                                    <!-- <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2103.10284">Paper</a>
                                        <a href="https://github.com/goodproj13/SG-Net">Code</a>
                                    </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/DenserNet.png" /></div> -->
                            <div class="paper-info">
                                <b> DenserNet: Weakly Supervised Visual Localization Using Multi-scale
                                    Features</b></br>
                                <i><b>Dongfang Liu</b>, Yiming Cui, Baijian Yang, and Yingjie Chen.</br>
                                    <b>AAAI 2021</b></br>
                                    <!-- <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2012.02366">Paper</a>
                                        <a href="https://github.com/goodproj13/DenserNet">Code</a>
                                    </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/Vectorbased.png" /></div> -->
                            <div class="paper-info">
                                <b> A Vector-Based Representation to Enhance Head Pose Estimation</b></br>
                                <i>Zhiwen Cao, Zongcheng Chu, <b>Dongfang Liu</b>, and Yingjie Chen.</br>
                                    <b>WACV 2021</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://arxiv.org/abs/2010.07184">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/Hierarchical.png" /></div> -->
                            <div class="paper-info">
                                <b> Hierarchical Attention Fusion for Geo-Localization</b></br>
                                <i>Liqi Yan, Yiming Cui, Yingjie Chen, and <b>Dongfang Liu</b>.</br>
                                    <b>ICASSP 2021</b></br>
                                    <!-- <div class="paper-link">
                                        <a href="https://arxiv.org/abs/2102.09186">Paper</a>
                                        <a href="https://github.com/ylqi/HAF">Code</a>
                                    </div> -->
                            </div>
                        </div>
                        <div class="paper-year">2020</div>
                        <div class="paper-item">
                            <!-- <div class="paper-image"><img src="img/MultimodalAgg.png" /></div> -->
                            <div class="paper-info">
                                <b> Multimodal Aggregation Approach for Memory Vision-Voice Indoor Navigation with
                                    Meta-Learning</b></br>
                                <i>Liqi Yan#, <b>Dongfang Liu#</b>, and Changbin Yu. (# first coauthor).</br>
                                    <b>IROS 2020</b></br>
                                    <!-- <div class="paper-link"> -->
                                        <!-- <a href="https://arxiv.org/abs/2009.00402">Paper</a> -->
                                        <!-- <a href="">Code</a> -->
                                    <!-- </div> -->
                            </div>
                        </div>
                    </section>
                </div>
                </div>
                <hr>

                <!-- Service -->
                <!-- <div class="page-section" id="services">
                    <h4 class="widget-title">Services</h4>
                    <div class="row">
                        <div class="col-md-12">
                            <div class="project-item">
                                <ul>
                                    <li><b>Associate editor</b>: IEEE Transactions on Circuits and Systems for
                                        Video Technology (TCSVT), 2023-</li>
                                    <li><b>Senior PC</b>: The International Joint Conference on Artificial
                                        Intelligence (IJCAI), 2023
                                    <li><b>Senior PC</b>: The Association for the Advancement of Artificial
                                        Intelligence (AAAI), 2023
                                    <li><b>Reviewer</b>: The Conference on Computer Vision and Pattern
                                        Recognition (CVPR), 2020-</li>
                                    <li><b>Reviewer</b>: The European Conference on Computer Vision (ECCV),
                                        2020-</li>
                                    <li><b>Reviewer</b>: The International Conference on Computer Vision (ICCV),
                                        2021-</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>  -->
                <!-- .contact-form -->
                <!-- <hr> -->

                <div class="row" id="footer">
                    <div class="col-md-12 text-center">
                        <script type='text/javascript' id='clustrmaps'
                            src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=n&d=hzX_uL3VVw4MumSglN7Q5_qHn2xgfao5PAYGLiieOCY'></script>
                        <!--<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=ubcKPhBPgXUfkNsB2U63LQjLkn3-FjAMmvrqlRrU9WU&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>-->
                        <p>The template of this website is designed by <a
                                href="http://www.templatemo.com/live/templatemo_441_volton">templatemo</a>.</p>
                        <!--<p>The template of this website is designed by <a href="https://clustrmaps.com/profile/1bfmz/widget/code/globe">templatemo</a>.</p>-->
                    </div>
                </div>

            </div>

        </div>
    </div>

    <script src="js/vendor/jquery-1.10.2.min.js"></script>
    <script src="js/min/plugins.min.js"></script>
    <script src="js/min/main.min.js"></script>
    <script type='text/javascript'>
        var readMorePapers = document.getElementById('readMore'),
            readMoreProjects = document.getElementById('readMoreProjects')
        function moreClick(ele, type) {
            var contentSection = document.getElementById(type),
                list = contentSection.getElementsByClassName('paper-more')[0]
            list.style.display = list.style.display === 'block' ? 'none' : 'block'
            var arrow = ele.getElementsByTagName('i')[0]
            console.log(arrow, 'arrowarrow')
            arrow.className = arrow.className === 'down' ? 'up' : 'down'
        }
        function publicationsClickEvent() {
            moreClick(readMorePapers, 'publications')
        }
        function projectsClickEvent() {
            moreClick(readMoreProjects, 'projects')
        }
        function bindEvent() {
            readMorePapers.addEventListener('click', publicationsClickEvent, false)
            readMoreProjects.addEventListener('click', projectsClickEvent, false)
        }
        bindEvent()
    </script>
    <script type="text/javascript" src="components/tab/tab.js"></script>
    <script type="text/javascript">
        var tab = new Tab({
            tabItem :'tab-item',
            pageItem: 'page-item',
            cur: 'cur',
            active: 'active'
        })
    </script>
</body>

</html>
